{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import datetime\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from absl import app, flags, logging\n",
    "import flax\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "import jax\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "from ml_collections import config_flags, ConfigDict\n",
    "import importlib\n",
    "from octo.model.components.tokenizers import BinTokenizer, LowdimObsTokenizer, ImageTokenizer, UnsqueezingImageTokenizer, ProjectionTokenizer, SiglipTokenizer\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "from octo.model.components.vit_encoders import SmallStem16\n",
    "from octo.data.dataset import make_single_dataset\n",
    "from octo.model.octo_model import OctoModel\n",
    "from octo.utils.jax_utils import initialize_compilation_cache\n",
    "from octo.utils.spec import ModuleSpec\n",
    "from octo.utils.train_callbacks import (\n",
    "    RolloutVisualizationCallback,\n",
    "    SaveCallback,\n",
    "    ValidationCallback,\n",
    "    VisualizationCallback,\n",
    ")\n",
    "from octo.utils.train_utils import (\n",
    "    check_config_diff,\n",
    "    create_optimizer,\n",
    "    format_name_with_config,\n",
    "    merge_params,\n",
    "    process_text,\n",
    "    Timer,\n",
    "    TrainState,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from jax_smi import initialise_tracking  # type: ignore\n",
    "\n",
    "    initialise_tracking()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2024-06-07 04:36:43.699365: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-07 04:36:43.699426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-07 04:36:43.701219: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-07 04:36:45.177355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# flags.DEFINE_string(\"name\", \"experiment\", \"Experiment name.\")\n",
    "# flags.DEFINE_bool(\"debug\", False, \"Debug config (no wandb logging)\")\n",
    "\n",
    "# # default_config_file = os.path.join(\n",
    "# #     os.path.dirname(__file__), \"configs/finetune_config.py\"\n",
    "# # )\n",
    "# config_flags.DEFINE_config_file(\n",
    "#     \"config\",\n",
    "#     \"/home/joshwajones/octo_digit/scripts/configs/josh_finetune_config.py:None\",\n",
    "#     \"File path to the training hyperparameter configuration.\",\n",
    "#     lock_config=False,\n",
    "# )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from ml_collections import ConfigDict\n",
    "from ml_collections.config_dict import FieldReference, placeholder\n",
    "from octo.model.components.action_heads import MSEActionHead\n",
    "from octo.model.components.tokenizers import BinTokenizer, LowdimObsTokenizer, ImageTokenizer, UnsqueezingImageTokenizer, ProjectionTokenizer, SiglipTokenizer\n",
    "from octo.model.components.vit_encoders import SmallStem16\n",
    "\n",
    "from octo.utils.spec import ModuleSpec\n",
    "\n",
    "def get_config(config_string=None):\n",
    "    # config_string = \"full,language_conditioned\"\n",
    "    config_string = \"full,multimodal\"\n",
    "    mode, task = config_string.split(\",\")\n",
    "    assert task in [\"image_conditioned\", \"language_conditioned\", \"multimodal\"]\n",
    "    assert mode in [\"full\", \"head_only\", \"head_mlp_only\"]\n",
    "\n",
    "    # Fill this in for your own dataset!\n",
    "\n",
    "    # There should be two image keys\n",
    "    # first image key should be the third-person view (None if not used)\n",
    "    # and second image key should be the wrist view (None if not used)\n",
    "    CAMS_ONLY = True \n",
    "\n",
    "    FINETUNING_KWARGS = {\n",
    "        \"name\": \"digit_dataset:8.8.0\",\n",
    "        # \"data_dir\": \"gs://619c8f721786ba/\",\n",
    "        \"data_dir\": \"/home/joshwajones/tensorflow_datasets/\",\n",
    "        \"image_obs_keys\": {\n",
    "            \"primary\": \"image_0\",\n",
    "            \"wrist\": \"image_1\",\n",
    "        },\n",
    "        \"digit_obs_keys\": {}, # TODO: remove this, treating digits just as images\n",
    "        \"proprio_obs_key\": None, # \"state\",\n",
    "        \"sensor_obs_keys\": {},\n",
    "        \"language_key\": \"language_instruction\",\n",
    "        # We want to avoid normalizing the gripper\n",
    "        \"action_normalization_mask\": [True, True, True, True, True, True, False],\n",
    "        # standardize_fn is dynamically loaded from a file\n",
    "        # for example: \"experiments/kevin/custom_standardization_transforms.py:aloha_dataset_transform\"\n",
    "        \"standardize_fn\": ModuleSpec.create(\n",
    "            \"octo.data.oxe.oxe_standardization_transforms:bridge_dataset_transform\",\n",
    "        ),\n",
    "        \n",
    "        # If the default data loading speed is too slow, try these:\n",
    "        # \"num_parallel_reads\": 8,  # for reading from disk / GCS\n",
    "        # \"num_parallel_calls\": 16,  # for initial dataset construction\n",
    "    }\n",
    "    if not CAMS_ONLY: \n",
    "        digit_names = { \n",
    "            \"digit_left\": \"digit_0\",\n",
    "            \"digit_right\": \"digit_1\",   \n",
    "        }\n",
    "        FINETUNING_KWARGS[\"image_obs_keys\"].update(digit_names)\n",
    "\n",
    "        sensor_names = { \n",
    "            \"spectro\": \"mel_spectro\",\n",
    "            \"imu\": \"imu\",\n",
    "            \"digit_0_embedding\": \"digit_0_embedding\", \n",
    "            \"digit_1_embedding\": \"digit_1_embedding\",\n",
    "        }\n",
    "        FINETUNING_KWARGS[\"sensor_obs_key\"].update(sensor_names)\n",
    "\n",
    "\n",
    "\n",
    "    # NEW_OBS_TOKENIZERS = { \n",
    "    #     \"digits\": \n",
    "    #         ModuleSpec.create(\n",
    "    #             ImageTokenizer,\n",
    "    #             obs_stack_keys=[\"image_digit_left\", \"image_digit_right\"],\n",
    "    #             task_stack_keys=[],\n",
    "    #             encoder=ModuleSpec.create(SmallStem16),\n",
    "    #         ),\n",
    "    #     \"spectrogram\": \n",
    "    #         ModuleSpec.create( \n",
    "    #             UnsqueezingImageTokenizer,\n",
    "    #             obs_stack_keys = [\"spectro\"], \n",
    "    #             task_stack_keys=[], \n",
    "    #             encoder=ModuleSpec.create(SmallStem16),\n",
    "    #         ),\n",
    "    #     \"imu\": ModuleSpec.create( \n",
    "    #             ProjectionTokenizer,\n",
    "    #             num_output_tokens=7,\n",
    "    #             n_bins=256,\n",
    "    #             obs_keys=[\"imu\"], \n",
    "    #         ),\n",
    "    #    \"digit_embeddings\": ModuleSpec.create( \n",
    "    #             ProjectionTokenizer,\n",
    "    #             num_output_tokens=7,\n",
    "    #             n_bins=256,\n",
    "    #             obs_keys=[\"digit_0_embedding\", \"digit_1_embedding\"], \n",
    "    #         ),\n",
    "    #     \"siglip\": {\n",
    "    #             \"freeze\": False, # True, \n",
    "    #             \"config\": { \n",
    "    #                 'encoder_path':'/home/sjosh/nfs/octo_digit/siglip.npz:img', # '/home/joshwajones/octo/siglip.npz:img',\n",
    "    #                 'image_model': 'vit', \n",
    "    #                 'image': dict(variant='B/16', pool_type='map')\n",
    "    #             }\n",
    "    #         }, \n",
    "    # }\n",
    "    NEW_OBS_TOKENIZERS = {}\n",
    "\n",
    "    NEW_ACTION_HEAD = None \n",
    "    NEW_ACTION_HEAD = ModuleSpec.create(\n",
    "        MSEActionHead,\n",
    "        readout_key=\"readout_action\",\n",
    "        use_map = False, # should this be disabled? \n",
    "        action_horizon=4,\n",
    "        action_dim=7\n",
    "    )\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    if mode == \"full\":\n",
    "        frozen_keys = None\n",
    "    elif mode == \"head_only\":\n",
    "        frozen_keys = (\"octo_transformer.*\",)\n",
    "    elif mode == \"head_mlp_only\":\n",
    "        frozen_keys = (\n",
    "            \"octo_transformer.*\",\n",
    "            \"heads_*.map_head.probe\",\n",
    "            \"heads_*.map_head.MultiHeadDotProductAttention_0.*\",\n",
    "        )\n",
    "    elif mode == \"frozen_transformer\":\n",
    "        frozen_keys = (\"octo_transformer.BlockTransformer_0.*\",)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode\")\n",
    "\n",
    "    max_steps = FieldReference(50000)\n",
    "    # max_steps = FieldReference(25000)\n",
    "    window_size = FieldReference(default=2)\n",
    "\n",
    "    config = dict(\n",
    "        pretrained_path=\"hf://rail-berkeley/octo-small\",\n",
    "        batch_size=256,\n",
    "        shuffle_buffer_size=10000,\n",
    "        num_steps=max_steps,\n",
    "        log_interval=100,\n",
    "        eval_interval=200,\n",
    "        save_interval=5000,\n",
    "        # save_dir=\"/home/joshwajones/octo_save_dir/\",\n",
    "        save_dir=\"gs://619c8f721786ba/octo_ckpts/\",\n",
    "\tseed=42,\n",
    "        wandb=dict(\n",
    "            project=\"octo\", group=placeholder(str), entity=placeholder(str)\n",
    "        ),\n",
    "        dataset_kwargs=FINETUNING_KWARGS,\n",
    "        modality=task,\n",
    "        finetuning_mode=mode,\n",
    "        window_size=window_size,\n",
    "        optimizer=dict(\n",
    "            learning_rate=dict(\n",
    "                name=\"cosine\",\n",
    "                init_value=0.0,\n",
    "                peak_value=3e-4,\n",
    "                warmup_steps=2000,\n",
    "                decay_steps=max_steps,\n",
    "                end_value=0.0,\n",
    "            ),\n",
    "            weight_decay=0.01,\n",
    "            clip_gradient=1.0,\n",
    "            frozen_keys=frozen_keys,\n",
    "            grad_accumulation_steps=None,  # if you are using grad accumulation, you need to adjust max_steps accordingly\n",
    "        ),\n",
    "        val_kwargs=dict(\n",
    "            val_shuffle_buffer_size=1000,\n",
    "            num_val_batches=16,\n",
    "        ),\n",
    "        viz_kwargs=dict(\n",
    "            eval_batch_size=64,\n",
    "            trajs_for_metrics=100,\n",
    "            trajs_for_viz=8,\n",
    "            samples_per_state=8,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if \"siglip\" in NEW_OBS_TOKENIZERS: \n",
    "        should_freeze, siglip_cfg = NEW_OBS_TOKENIZERS[\"siglip\"][\"freeze\"], NEW_OBS_TOKENIZERS[\"siglip\"][\"config\"]\n",
    "        config[\"siglip_config\"] = siglip_cfg\n",
    "        NEW_OBS_TOKENIZERS[\"siglip\"] = ModuleSpec.create( \n",
    "                    SiglipTokenizer,\n",
    "                    image=siglip_cfg[\"image\"],\n",
    "                    image_model=siglip_cfg[\"image_model\"],\n",
    "                    encoder_path=siglip_cfg[\"encoder_path\"],\n",
    "                    n_bins=256,\n",
    "                    obs_keys=['siglip'],\n",
    "        )\n",
    "\n",
    "        if should_freeze: \n",
    "            prev_frozen = frozen_keys if frozen_keys else ()\n",
    "            config[\"optimizer\"][\"frozen_keys\"] = prev_frozen + (\"octo_transformer.observation_tokenizers_siglip.*\", \"*hf_model*\")\n",
    "\n",
    "\n",
    "    if task == \"image_conditioned\":\n",
    "        goal_relabeling_strategy = \"uniform\"\n",
    "        keep_image_prob = 1.0\n",
    "    elif task == \"language_conditioned\":\n",
    "        goal_relabeling_strategy = None\n",
    "        keep_image_prob = 0.0\n",
    "    elif task == \"multimodal\":\n",
    "        goal_relabeling_strategy = \"uniform\"\n",
    "        keep_image_prob = 0.5\n",
    "    else:\n",
    "        raise ValueError(\"Invalid modality\")\n",
    "\n",
    "\n",
    "    traj_transform_kwargs = dict(\n",
    "        window_size=window_size,\n",
    "        action_horizon=4,\n",
    "        goal_relabeling_strategy=goal_relabeling_strategy,\n",
    "        task_augment_strategy=\"delete_task_conditioning\",\n",
    "        task_augment_kwargs=dict(\n",
    "            keep_image_prob=keep_image_prob,\n",
    "        ),\n",
    "        # If the default data loading speed is too slow, try these:\n",
    "        # num_parallel_calls=16,  # for less CPU-intensive ops\n",
    "    )\n",
    "\n",
    "    workspace_augment_kwargs = dict(\n",
    "        random_resized_crop=dict(scale=[0.8, 1.0], ratio=[0.9, 1.1]),\n",
    "        random_brightness=[0.1],\n",
    "        random_contrast=[0.9, 1.1],\n",
    "        random_saturation=[0.9, 1.1],\n",
    "        random_hue=[0.05],\n",
    "        augment_order=[\n",
    "            \"random_resized_crop\",\n",
    "            \"random_brightness\",\n",
    "            \"random_contrast\",\n",
    "            \"random_saturation\",\n",
    "            \"random_hue\",\n",
    "        ],\n",
    "    )\n",
    "    wrist_augment_kwargs = dict(\n",
    "        random_brightness=[0.1],\n",
    "        random_contrast=[0.9, 1.1],\n",
    "        random_saturation=[0.9, 1.1],\n",
    "        random_hue=[0.05],\n",
    "        augment_order=[\n",
    "            \"random_brightness\",\n",
    "            \"random_contrast\",\n",
    "            \"random_saturation\",\n",
    "            \"random_hue\",\n",
    "        ],\n",
    "    )\n",
    "    digit_augment_kwargs = dict(\n",
    "        random_resized_crop=dict(scale=[0.8, 1.0], ratio=[0.9, 1.1]),\n",
    "        augment_order=[\n",
    "            \"random_resized_crop\"\n",
    "        ],\n",
    "    )\n",
    "    frame_transform_kwargs = dict(\n",
    "        resize_size={\n",
    "            \"primary\": (256, 256),  # workspace (3rd person) camera is at 256x256\n",
    "            \"wrist\": (128, 128),   # wrist camera is at 128x128\n",
    "            \"digit_left\": (256, 256), #(128, 128),\n",
    "            \"digit_right\": (256, 256),  # (128, 128)\n",
    "        },\n",
    "        image_augment_kwargs = { \n",
    "            \"primary\": workspace_augment_kwargs, \n",
    "            \"wrist\": wrist_augment_kwargs, \n",
    "            \"digit_left\": digit_augment_kwargs,\n",
    "            \"digit_right\": digit_augment_kwargs,\n",
    "        }\n",
    "    )\n",
    "    # If the default data loading speed is too slow, try these:\n",
    "    config[\n",
    "        \"frame_transform_threads\"\n",
    "    ] = 16  # for the most CPU-intensive ops (decoding, resizing, augmenting)\n",
    "\n",
    "    config[\"traj_transform_kwargs\"] = traj_transform_kwargs\n",
    "    config[\"frame_transform_kwargs\"] = frame_transform_kwargs\n",
    "    config[\"new_obs_tokenizers\"] = NEW_OBS_TOKENIZERS\n",
    "\n",
    "    config['update_config'] = { \n",
    "        \"model\":  {\n",
    "            \"repeat_task_tokens\": True,\n",
    "        }\n",
    "    }\n",
    "    if NEW_ACTION_HEAD is not None: \n",
    "        config['update_config']['model']['heads'] = { \n",
    "            'action': ConfigDict(NEW_ACTION_HEAD)\n",
    "        } \n",
    "    config['update_config'] = ConfigDict(config['update_config'])\n",
    "\n",
    "    return ConfigDict(config)\n",
    "\n",
    "CONFIG = get_config(None)\n",
    "FLAGS = { \n",
    "    \"name\": \"experiment\", \n",
    "    \"debug\": True, \n",
    "    \"config\": CONFIG, \n",
    "} \n",
    "FLAGS = ConfigDict(FLAGS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "MAX_KEY_LEN = 15\n",
    "INDENT_SIZE = MAX_KEY_LEN + 4\n",
    "INDENT = ''.join([' ' for _ in range(INDENT_SIZE)])\n",
    "def recursive_dict_print(dictionary, prefix=\"\"): \n",
    "    for key, val in dictionary.items(): \n",
    "        key = key[:MAX_KEY_LEN]\n",
    "        if isinstance(val, dict): \n",
    "            print(f'{prefix}{key}')\n",
    "            new_prefix = prefix + INDENT\n",
    "            recursive_dict_print(val, new_prefix)\n",
    "        else: \n",
    "            indent = ''.join([' ' for _ in range(INDENT_SIZE - len(key))])\n",
    "            print(f'{prefix}{key}:{indent}{val.shape}')\n",
    "\n",
    "\n",
    "\n",
    "initialize_compilation_cache()\n",
    "devices = jax.devices()\n",
    "logging.info(\n",
    "    f\"\"\"\n",
    "    Octo Finetuning Script\n",
    "    ======================\n",
    "    Pretrained model: {FLAGS.config.pretrained_path}\n",
    "    Finetuning Dataset: {FLAGS.config.dataset_kwargs.name}\n",
    "    Data dir: {FLAGS.config.dataset_kwargs.data_dir}\n",
    "    Task Modality: {FLAGS.config.modality}\n",
    "    Finetuning Mode: {FLAGS.config.finetuning_mode}\n",
    "\n",
    "    # Devices: {jax.device_count()}\n",
    "    Batch size: {FLAGS.config.batch_size} ({FLAGS.config.batch_size // len(devices) } per device)\n",
    "    # Steps: {FLAGS.config.num_steps}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "#########\n",
    "#\n",
    "# Setup Jax Data Parallelism\n",
    "#\n",
    "#########\n",
    "\n",
    "assert (\n",
    "    FLAGS.config.batch_size % len(devices) == 0\n",
    "), f\"Batch size ({FLAGS.config.batch_size}) must be divisible by the number of devices ({len(devices)})\"\n",
    "assert (\n",
    "    FLAGS.config.viz_kwargs.eval_batch_size % len(devices) == 0\n",
    "), f\"Eval batch size ({FLAGS.config.viz_kwargs.eval_batch_size}) must be divisible by the number of devices ({len(devices)})\"\n",
    "\n",
    "# create a 1D mesh with a single axis named \"batch\"\n",
    "mesh = Mesh(jax.devices(), axis_names=\"batch\")\n",
    "# Our batches will be data-parallel sharded -- each device will get a slice of the batch\n",
    "dp_sharding = NamedSharding(mesh, PartitionSpec(\"batch\"))\n",
    "# Our model will be replicated across devices (we are only doing data parallelism, not model parallelism)\n",
    "replicated_sharding = NamedSharding(mesh, PartitionSpec())\n",
    "\n",
    "# prevent tensorflow from using GPU memory since it's only used for data loading\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "#########\n",
    "#\n",
    "# Setup WandB\n",
    "#\n",
    "#########\n",
    "\n",
    "name = format_name_with_config(\n",
    "    FLAGS.name,\n",
    "    FLAGS.config.to_dict(),\n",
    ")\n",
    "# wandb_id = \"{name}_{time}\".format(\n",
    "#     name=name,\n",
    "#     time=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "# )\n",
    "# wandb.init(\n",
    "#     config=FLAGS.config.to_dict(),\n",
    "#     id=wandb_id,\n",
    "#     name=name,\n",
    "#     mode=\"disabled\" if FLAGS.debug else None,\n",
    "#     **FLAGS.config.wandb,\n",
    "# )\n",
    "\n",
    "#########\n",
    "#\n",
    "# Load Pretrained model + optionally modify config\n",
    "#\n",
    "#########\n",
    "pretrained_model_kwargs = {\n",
    "    \"checkpoint_path\": \"/home/joshwajones/tpu_octo_ckpts/standard_largebatch_20240605_015537\"\n",
    "    # \"checkpoint_path\": FLAGS.config.pretrained_path\n",
    "}\n",
    "for step in range(5000, 50001, 5000):\n",
    "        pretrained_model_kwargs[\"step\"] = step \n",
    "pretrained_model = OctoModel.load_pretrained(\n",
    "    **pretrained_model_kwargs\n",
    ")\n",
    "rng = jax.random.PRNGKey(FLAGS.config.seed)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "model = pretrained_model\n",
    "\n",
    "flat_config = flax.traverse_util.flatten_dict(\n",
    "    pretrained_model.config, keep_empty_nodes=True\n",
    ")\n",
    "\n",
    "config = ConfigDict(flax.traverse_util.unflatten_dict(flat_config))\n",
    "# config.update(FLAGS.config.get(\"update_config\", ConfigDict()))\n",
    "config = config.to_dict()\n",
    "# check_config_diff(config, pretrained_model.config)\n",
    "#########\n",
    "#\n",
    "# Setup Data Loader\n",
    "#\n",
    "#########\n",
    "\n",
    "# create text processor\n",
    "if config[\"text_processor\"] is None:\n",
    "    text_processor = None\n",
    "else:\n",
    "    text_processor = ModuleSpec.instantiate(config[\"text_processor\"])()\n",
    "\n",
    "def process_batch(batch):\n",
    "    batch = process_text(batch, text_processor)\n",
    "    del batch[\"dataset_name\"]\n",
    "    return batch\n",
    "\n",
    "params = model.params\n",
    "if FLAGS.config.optimizer.frozen_keys is None:\n",
    "    FLAGS.config.optimizer.frozen_keys = model.config[\"optimizer\"][\"frozen_keys\"]\n",
    "\n",
    "tx, lr_callable, param_norm_callable = create_optimizer(\n",
    "    params,\n",
    "    **FLAGS.config.optimizer.to_dict(),\n",
    ")\n",
    "train_state = TrainState.create(\n",
    "    model=model,\n",
    "    tx=tx,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "if FLAGS.config.modality == \"image_conditioned\":\n",
    "    modes_to_evaluate = [\"image_conditioned\"]\n",
    "elif FLAGS.config.modality == \"text_conditioned\":\n",
    "    modes_to_evaluate = [\"text_conditioned\"]\n",
    "elif FLAGS.config.modality == \"multimodal\":\n",
    "    modes_to_evaluate = [\"image_conditioned\", \"text_conditioned\"]\n",
    "else:\n",
    "    modes_to_evaluate = [\"base\"]\n",
    "\n",
    "dataset_kwargs_list = [FLAGS.config.dataset_kwargs]\n",
    "\n",
    "# viz_callback = VisualizationCallback(\n",
    "#     text_processor=text_processor,\n",
    "#     val_dataset_kwargs_list=dataset_kwargs_list,\n",
    "#     dataset_kwargs=FLAGS.config,\n",
    "#     modes_to_evaluate=modes_to_evaluate,\n",
    "#     **FLAGS.config.viz_kwargs,\n",
    "# )\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:jax._src.compilation_cache:Cache already previously initialized at /home/joshwajones/.jax_compilation_cache\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# create text processor\n",
    "if config[\"text_processor\"] is None:\n",
    "    text_processor = None\n",
    "else:\n",
    "    text_processor = ModuleSpec.instantiate(config[\"text_processor\"])()\n",
    "\n",
    "def process_batch(batch):\n",
    "    batch = process_text(batch, text_processor)\n",
    "    del batch[\"dataset_name\"]\n",
    "    return batch\n",
    "\n",
    "dataset = make_single_dataset(\n",
    "    FLAGS.config.dataset_kwargs,\n",
    "    traj_transform_kwargs=FLAGS.config.traj_transform_kwargs,\n",
    "    frame_transform_kwargs=FLAGS.config.frame_transform_kwargs,\n",
    "    train=True,\n",
    ")\n",
    "train_data_iter = (\n",
    "    dataset.repeat()\n",
    "    .unbatch()\n",
    "    .shuffle(FLAGS.config.shuffle_buffer_size)\n",
    "    .batch(FLAGS.config.batch_size)\n",
    "    .iterator()\n",
    ")\n",
    "train_data_iter = map(process_batch, train_data_iter)\n",
    "example_batch = next(train_data_iter)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "# FLAGS.config.viz_kwargs['trajs_for_viz'] = 8\n",
    "# FLAGS.config.viz_kwargs['samples_per_state'] = 8\n",
    "# FLAGS.config.viz_kwargs['trajs_for_metrics'] = 1\n",
    "# viz_callback = VisualizationCallback(\n",
    "#     text_processor=text_processor,\n",
    "#     val_dataset_kwargs_list=dataset_kwargs_list,\n",
    "#     dataset_kwargs=FLAGS.config,\n",
    "#     modes_to_evaluate=modes_to_evaluate,\n",
    "#     **FLAGS.config.viz_kwargs,\n",
    "# )\n",
    "# viz_metrics = viz_callback(train_state, 0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _gcd_import at 0x7f2f1fa6b400> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x7f2f1fa6b400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _gcd_import at 0x7f2f1fa6b400> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x7f2f1fa6b400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function _gcd_import at 0x7f2f1fa6b400> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x7f2f1fa6b400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root:'tasks' contains extra items compared to example_batch: {'task_completed', 'timestep_pad_mask'}\n",
      "WARNING:root:embodiment_action_dim is highly recommended for diffusion action head if any action dimensions were masked during training\n",
      "  0%|          | 0/1 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unnormalize() missing 1 required positional argument: 'mask'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m FLAGS\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mviz_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrajs_for_metrics\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m viz_callback \u001b[38;5;241m=\u001b[39m VisualizationCallback(\n\u001b[1;32m      5\u001b[0m     text_processor\u001b[38;5;241m=\u001b[39mtext_processor,\n\u001b[1;32m      6\u001b[0m     val_dataset_kwargs_list\u001b[38;5;241m=\u001b[39mdataset_kwargs_list,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mFLAGS\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mviz_kwargs,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m viz_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mviz_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/octo_digit/octo/utils/train_callbacks.py:318\u001b[0m, in \u001b[0;36mVisualizationCallback.__call__\u001b[0;34m(self, train_state, step)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mode, policy_fn \u001b[38;5;129;01min\u001b[39;00m modal_policy_fns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrajs_for_metrics \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 318\u001b[0m         raw_infos \u001b[38;5;241m=\u001b[39m \u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_evaluations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolicy_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_trajs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrajs_for_metrics\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m visualizer\u001b[38;5;241m.\u001b[39mmetrics_for_wandb(raw_infos)\n\u001b[1;32m    322\u001b[0m         wandb_metrics[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline_metrics_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metrics\n",
      "File \u001b[0;32m~/octo_digit/octo/utils/visualization_lib.py:249\u001b[0m, in \u001b[0;36mVisualizer.raw_evaluations\u001b[0;34m(self, policy_fn, max_trajs)\u001b[0m\n\u001b[1;32m    243\u001b[0m info \u001b[38;5;241m=\u001b[39m run_policy_on_trajectory(\n\u001b[1;32m    244\u001b[0m     policy_fn,\n\u001b[1;32m    245\u001b[0m     traj,\n\u001b[1;32m    246\u001b[0m     text_processor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_processor,\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# del info['proprio']\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43madd_unnormalized_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_proprio_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m info \u001b[38;5;241m=\u001b[39m add_manipulation_metrics(info)\n\u001b[1;32m    251\u001b[0m all_traj_info\u001b[38;5;241m.\u001b[39mappend(info)\n",
      "File \u001b[0;32m~/octo_digit/octo/utils/visualization_lib.py:412\u001b[0m, in \u001b[0;36madd_unnormalized_info\u001b[0;34m(info, normalization_stats)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_unnormalized_info\u001b[39m(\n\u001b[1;32m    396\u001b[0m     info,\n\u001b[1;32m    397\u001b[0m     normalization_stats,\n\u001b[1;32m    398\u001b[0m ):\n\u001b[1;32m    399\u001b[0m     info\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    400\u001b[0m         {\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munnorm_pred_actions\u001b[39m\u001b[38;5;124m\"\u001b[39m: unnormalize(\n\u001b[1;32m    402\u001b[0m                 info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_actions\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnormalization_stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    403\u001b[0m             ),\n\u001b[1;32m    404\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munnorm_pred_actions_chunk\u001b[39m\u001b[38;5;124m\"\u001b[39m: unnormalize(\n\u001b[1;32m    405\u001b[0m                 info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_actions_chunk\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnormalization_stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    406\u001b[0m             ),\n\u001b[1;32m    407\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munnorm_actions\u001b[39m\u001b[38;5;124m\"\u001b[39m: unnormalize(\n\u001b[1;32m    408\u001b[0m                 info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnormalization_stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    409\u001b[0m             ),\n\u001b[1;32m    410\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    411\u001b[0m                 {\n\u001b[0;32m--> 412\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munnorm_proprio\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43munnormalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m                        \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproprio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnormalization_stats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproprio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m                 }\n\u001b[1;32m    416\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproprio\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info\n\u001b[1;32m    417\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    418\u001b[0m             ),\n\u001b[1;32m    419\u001b[0m         }\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[0;31mTypeError\u001b[0m: unnormalize() missing 1 required positional argument: 'mask'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.14 64-bit ('octo': conda)"
  },
  "interpreter": {
   "hash": "f403d8570fe2ec26d1daafba18cc388bc7c4df4bdbb7e83b63f7ca08cd3052fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}